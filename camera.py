#!/usr/bin/env python3
"""
·ª®ng d·ª•ng Camera Real-time ƒë·ªÉ nh·∫≠n di·ªán c·∫£m x√∫c
S·ª≠ d·ª•ng webcam ƒë·ªÉ ph√¢n t√≠ch c·∫£m x√∫c theo th·ªùi gian th·ª±c
"""

import cv2
import numpy as np
import sys
import os
import time
from datetime import datetime
from src.main.emotion_detection import EmotionDetectionSystem

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import modules
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'main'))

try:
    from src.main.emotion_detection import EmotionDetectionSystem
    print("‚úÖ Import EmotionDetectionSystem th√†nh c√¥ng")
except ImportError as e:
    print(f"‚ùå L·ªói import EmotionDetectionSystem: {e}")
    print("üí° H√£y ƒë·∫£m b·∫£o file emotion_detection.py t·ªìn t·∫°i trong th∆∞ m·ª•c src/main/")
    sys.exit(1)

class RealTimeEmotionDetection:
    """
    L·ªõp x·ª≠ l√Ω nh·∫≠n di·ªán c·∫£m x√∫c real-time qua camera
    """
    
    def __init__(self, camera_index=0, detector_backend="opencv"):
        """
        Kh·ªüi t·∫°o h·ªá th·ªëng nh·∫≠n di·ªán c·∫£m x√∫c real-time
        
        Args:
            camera_index: Index c·ªßa camera (0 l√† camera m·∫∑c ƒë·ªãnh)
            detector_backend: Backend ƒë·ªÉ detect khu√¥n m·∫∑t
        """
        self.camera_index = camera_index
        self.detector_backend = detector_backend
        self.emotion_system = EmotionDetectionSystem(detector_backend=detector_backend)
        
        # Kh·ªüi t·∫°o camera
        self.cap = cv2.VideoCapture(camera_index)
        if not self.cap.isOpened():
            raise ValueError(f"Kh√¥ng th·ªÉ m·ªü camera v·ªõi index {camera_index}")
        
        # C·∫•u h√¨nh camera
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        # Bi·∫øn ƒë·ªÉ l∆∞u tr·ªØ k·∫øt qu·∫£
        self.current_emotion = "Unknown"
        self.emotion_confidence = 0.0
        self.face_detected = False
        self.fps = 0
        self.frame_count = 0
        self.start_time = time.time()
        
        # M√†u s·∫Øc cho c√°c c·∫£m x√∫c
        self.emotion_colors = {
            'angry': (0, 0, 255),      # ƒê·ªè
            'disgust': (0, 128, 0),    # Xanh l√°
            'fear': (128, 0, 128),     # T√≠m
            'happy': (0, 255, 255),    # V√†ng
            'sad': (255, 0, 0),        # Xanh d∆∞∆°ng
            'surprise': (0, 255, 0),   # Xanh l√° s√°ng
            'neutral': (128, 128, 128) # X√°m
        }
        
        # Emoji cho c√°c c·∫£m x√∫c
        self.emotion_emojis = {
            'angry': 'üò†',
            'disgust': 'ü§¢',
            'fear': 'üò®',
            'happy': 'üòä',
            'sad': 'üò¢',
            'surprise': 'üò≤',
            'neutral': 'üòê'
        }
        
        print(f"‚úÖ Camera kh·ªüi t·∫°o th√†nh c√¥ng (Index: {camera_index})")
        print(f"üìπ ƒê·ªô ph√¢n gi·∫£i: {int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))}x{int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))}")
        print(f"üéØ Detector: {detector_backend}")
    
    def analyze_frame(self, frame):
        """
        Ph√¢n t√≠ch c·∫£m x√∫c t·ª´ frame
        
        Args:
            frame: Frame t·ª´ camera
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ ph√¢n t√≠ch
        """
        try:
            # L∆∞u frame t·∫°m th·ªùi
            temp_path = "temp_frame.jpg"
            cv2.imwrite(temp_path, frame)
            
            # Ph√¢n t√≠ch c·∫£m x√∫c
            result = self.emotion_system.analyze_emotion_from_image(temp_path)
            
            # X√≥a file t·∫°m
            if os.path.exists(temp_path):
                os.remove(temp_path)
            
            if result.get('success', False) and result.get('results'):
                results = result['results']
                if 'dominant_emotion' in results:
                    self.current_emotion = results['dominant_emotion']
                    if 'emotion' in results and self.current_emotion in results['emotion']:
                        self.emotion_confidence = results['emotion'][self.current_emotion]
                    self.face_detected = True
                else:
                    self.face_detected = False
            else:
                self.face_detected = False
            
            return result
            
        except Exception as e:
            print(f"‚ùå L·ªói khi ph√¢n t√≠ch frame: {e}")
            self.face_detected = False
            return None
    
    def draw_emotion_info(self, frame):
        """
        V·∫Ω th√¥ng tin c·∫£m x√∫c l√™n frame
        
        Args:
            frame: Frame c·∫ßn v·∫Ω th√¥ng tin
        """
        # T√≠nh FPS
        self.frame_count += 1
        elapsed_time = time.time() - self.start_time
        if elapsed_time > 0:
            self.fps = self.frame_count / elapsed_time
        
        # V·∫Ω background cho th√¥ng tin
        info_height = 120
        overlay = frame.copy()
        cv2.rectangle(overlay, (0, 0), (frame.shape[1], info_height), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # Th√¥ng tin c∆° b·∫£n
        cv2.putText(frame, f"FPS: {self.fps:.1f}", (10, 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.putText(frame, f"Detector: {self.detector_backend}", (10, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # Th√¥ng tin c·∫£m x√∫c
        if self.face_detected:
            emotion_color = self.emotion_colors.get(self.current_emotion, (255, 255, 255))
            emoji = self.emotion_emojis.get(self.current_emotion, 'üòê')
            
            # V·∫Ω c·∫£m x√∫c ch√≠nh
            emotion_text = f"Emotion: {emoji} {self.current_emotion.upper()}"
            cv2.putText(frame, emotion_text, (10, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, emotion_color, 2)
            
            # V·∫Ω ƒë·ªô tin c·∫≠y
            confidence_text = f"Confidence: {self.emotion_confidence:.2f}"
            cv2.putText(frame, confidence_text, (10, 105), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        else:
            cv2.putText(frame, "No face detected", (10, 80), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)
        
        # V·∫Ω h∆∞·ªõng d·∫´n
        cv2.putText(frame, "Press 'q' to quit, 's' to save frame", (frame.shape[1] - 350, 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    def save_frame(self, frame):
        """
        L∆∞u frame hi·ªán t·∫°i
        
        Args:
            frame: Frame c·∫ßn l∆∞u
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"camera_frame_{timestamp}.jpg"
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs("camera_captures", exist_ok=True)
        filepath = os.path.join("camera_captures", filename)
        
        cv2.imwrite(filepath, frame)
        print(f"üì∏ ƒê√£ l∆∞u frame: {filepath}")
        
        # Ph√¢n t√≠ch v√† l∆∞u k·∫øt qu·∫£
        if self.face_detected:
            result = {
                "image_path": filepath,
                "analysis_time": datetime.now().isoformat(),
                "emotion": self.current_emotion,
                "confidence": self.emotion_confidence,
                "detector": self.detector_backend
            }
            
            result_file = filepath.replace(".jpg", "_result.json")
            with open(result_file, 'w', encoding='utf-8') as f:
                import json
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"üìÑ ƒê√£ l∆∞u k·∫øt qu·∫£: {result_file}")
    
    def run(self, analysis_interval=30):
        """
        Ch·∫°y ·ª©ng d·ª•ng camera real-time
        
        Args:
            analysis_interval: S·ªë frame gi·ªØa c√°c l·∫ßn ph√¢n t√≠ch (m·∫∑c ƒë·ªãnh: 30)
        """
        print("üé• B·∫Øt ƒë·∫ßu nh·∫≠n di·ªán c·∫£m x√∫c real-time...")
        print("üí° H∆∞·ªõng d·∫´n:")
        print("   - Nh·∫•n 'q' ƒë·ªÉ tho√°t")
        print("   - Nh·∫•n 's' ƒë·ªÉ l∆∞u frame hi·ªán t·∫°i")
        print("   - Nh·∫•n 'a' ƒë·ªÉ ph√¢n t√≠ch ngay l·∫≠p t·ª©c")
        print("   - Nh·∫•n 'd' ƒë·ªÉ thay ƒë·ªïi detector")
        
        frame_counter = 0
        
        try:
            while True:
                # ƒê·ªçc frame t·ª´ camera
                ret, frame = self.cap.read()
                if not ret:
                    print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc frame t·ª´ camera")
                    break
                
                # Ph√¢n t√≠ch c·∫£m x√∫c theo interval
                frame_counter += 1
                if frame_counter % analysis_interval == 0:
                    self.analyze_frame(frame)
                
                # V·∫Ω th√¥ng tin l√™n frame
                self.draw_emotion_info(frame)
                
                # Hi·ªÉn th·ªã frame
                cv2.imshow('Real-time Emotion Detection', frame)
                
                # X·ª≠ l√Ω ph√≠m nh·∫•n
                key = cv2.waitKey(1) & 0xFF
                
                if key == ord('q'):
                    print("üëã Tho√°t ·ª©ng d·ª•ng...")
                    break
                elif key == ord('s'):
                    self.save_frame(frame)
                elif key == ord('a'):
                    print("üîç Ph√¢n t√≠ch ngay l·∫≠p t·ª©c...")
                    self.analyze_frame(frame)
                elif key == ord('d'):
                    self.change_detector()
        
        except KeyboardInterrupt:
            print("\nüëã Tho√°t ·ª©ng d·ª•ng...")
        finally:
            self.cleanup()
    
    def change_detector(self):
        """
        Thay ƒë·ªïi detector backend
        """
        detectors = ["opencv", "mtcnn", "retinaface", "mediapipe"]
        current_index = detectors.index(self.detector_backend)
        next_index = (current_index + 1) % len(detectors)
        new_detector = detectors[next_index]
        
        print(f"üîÑ Thay ƒë·ªïi detector t·ª´ {self.detector_backend} sang {new_detector}")
        self.detector_backend = new_detector
        self.emotion_system = EmotionDetectionSystem(detector_backend=new_detector)
    
    def cleanup(self):
        """
        D·ªçn d·∫πp t√†i nguy√™n
        """
        if self.cap.isOpened():
            self.cap.release()
        cv2.destroyAllWindows()
        print("üßπ ƒê√£ d·ªçn d·∫πp t√†i nguy√™n")

def main():
    """
    H√†m ch√≠nh
    """
    print("üé≠ Camera Real-time Emotion Detection")
    print("=" * 50)
    
    # Ki·ªÉm tra camera
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("‚ùå Kh√¥ng th·ªÉ m·ªü camera. Vui l√≤ng ki·ªÉm tra:")
        print("   - Camera c√≥ ƒë∆∞·ª£c k·∫øt n·ªëi kh√¥ng?")
        print("   - Camera c√≥ ƒëang ƒë∆∞·ª£c s·ª≠ d·ª•ng b·ªüi ·ª©ng d·ª•ng kh√°c kh√¥ng?")
        print("   - Quy·ªÅn truy c·∫≠p camera c√≥ ƒë∆∞·ª£c c·∫•p kh√¥ng?")
        return
    cap.release()
    
    try:
        # Kh·ªüi t·∫°o h·ªá th·ªëng
        emotion_camera = RealTimeEmotionDetection(camera_index=0, detector_backend="opencv")
        
        # Ch·∫°y ·ª©ng d·ª•ng
        emotion_camera.run(analysis_interval=30)  # Ph√¢n t√≠ch m·ªói 30 frame
        
    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        print("üí° G·ª£i √Ω:")
        print("   - Ki·ªÉm tra camera c√≥ ho·∫°t ƒë·ªông kh√¥ng")
        print("   - Th·ª≠ thay ƒë·ªïi camera_index (0, 1, 2...)")
        print("   - Ki·ªÉm tra quy·ªÅn truy c·∫≠p camera")

if __name__ == "__main__":
    main() 